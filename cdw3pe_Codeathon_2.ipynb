{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RainaVardhan/Codeathon2/blob/main/cdw3pe_Codeathon_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4u4YC8iqxpe"
      },
      "source": [
        "# CODEATHON 2: Recognizing UVA landmarks with neural nets (50 pts)\n",
        "![UVA Grounds](https://giving.virginia.edu/sites/default/files/2019-02/jgi-teaser-image.jpg)\n",
        "\n",
        "The UVA Grounds is known for its Jeffersonian architecture and place in U.S. history as a model for college and university campuses throughout the country. Throughout its history, the University of Virginia has won praises for its unique Jeffersonian architecture.\n",
        "\n",
        "In this codeathon, you will attempt the build an image recognition system to classify different buildlings/landmarks on Grounds. You will earn 50 points for this codeathon plus 10 bonus points. To make it easier for you, some codes have been provided to help you process the data, you may modify it to fit your needs.\n",
        "\n",
        "You must submit the .ipynb file via UVA Collab with the following format: **yourUVAComputingID_codeathon_2.ipynb**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIAunzfaCNAH"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import sklearn\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whpECRG_B2Nj"
      },
      "source": [
        "# Step 1: Process the  Dataset\n",
        "The full dataset is huge (+37GB) with +13K images of 18 classes. So it will take a while to download, extract, and process. To save you time and effort, a subset of the data has been resized and compressed to only 379Mb and stored in my Firebase server. This dataset will be the one you will benchmark for your grade. If you are up for a challenge (and perhaps bonus points), contact the instructor for the full dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4E5kiDN9OFs"
      },
      "outputs": [],
      "source": [
        "# Download dataset from Firebase\n",
        "!wget https://firebasestorage.googleapis.com/v0/b/uva-landmark-images.appspot.com/o/dataset.zip?alt=media&token=e1403951-30d6-42b8-ba4e-394af1a2ddb7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKWszcAd9WJh"
      },
      "outputs": [],
      "source": [
        "# Extract content\n",
        "!unzip \"/content/dataset.zip?alt=media\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "474nVh3m-FrM"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_files\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "#from keras.utils import np_utils\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from tqdm import tqdm # progress bar\n",
        "\n",
        "data_dir = \"/content/dataset/\"\n",
        "batch_size = 32;\n",
        "# IMPORTANT: Depends on what pre-trained model you choose, you will need to change these dimensions accordingly\n",
        "img_height = 150;\n",
        "img_width = 150;\n",
        "\n",
        "# Training Dataset\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split = 0.2,\n",
        "    subset = \"training\",\n",
        "    seed = 42,\n",
        "    image_size= (img_height, img_width),\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n",
        "# Validation Dataset\n",
        "validation_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split = 0.2,\n",
        "    subset = \"validation\",\n",
        "    seed = 42,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7x_HSPOCk4uP"
      },
      "outputs": [],
      "source": [
        "type(train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gWDUhbDDFZn"
      },
      "outputs": [],
      "source": [
        "# Visualize some of the train samples of one batch\n",
        "# Make sure you create the class names that match the order of their appearances in the \"files\" variable\n",
        "class_names = ['AcademicalVillage', 'AldermanLibrary', 'AlumniHall', 'AquaticFitnessCenter',\n",
        "  'BavaroHall', 'BrooksHall', 'ClarkHall', 'MadisonHall', 'MinorHall', 'NewCabellHall',\n",
        "  'NewcombHall', 'OldCabellHall', 'OlssonHall', 'RiceHall', 'Rotunda', 'ScottStadium',\n",
        "  'ThorntonHall', 'UniversityChapel']\n",
        "\n",
        "# Rows and columns are set to fit one training batch (32)\n",
        "n_rows = 8\n",
        "n_cols = 4\n",
        "plt.figure(figsize=(n_cols * 3, n_rows * 3))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range (n_rows*n_cols):\n",
        "        plt.subplot(n_rows, n_cols, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[labels[i]], fontsize=12)\n",
        "plt.subplots_adjust(wspace=.2, hspace=.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP0-jzrMYioK"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE STARTS HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJmFciPVn3YY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L5V5__CnzWP"
      },
      "source": [
        "# Step 2: Create your own CNN architecture\n",
        "You must design your own architecture. To get started, you may get inspiration from one in CNN notebook  (i.e. use one similar to LeNet-5 or AlexNet). You will have to report the design of the architecture:\n",
        "\n",
        "1.   How many layers does it have?\n",
        "2.   Why do you decide on a certain number nodes per layer?\n",
        "3.   Which activation functions do you choose?\n",
        "4.   How many parameters does it has in total?\n",
        "\n",
        "Hint: use `myModel.summary()` to learn on the layers and parameters\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZfEgQIUnzWY"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "myModel = models.Sequential([layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
        "                             layers.MaxPooling2D((2, 2)),\n",
        "                             layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "                             layers.MaxPooling2D((2, 2)),\n",
        "                             layers.Flatten(),\n",
        "                             layers.Dense(128, activation='relu'),\n",
        "                             layers.Dropout(0.3),\n",
        "                             layers.Dense(len(class_names), activation='softmax')])\n",
        "myModel.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33s9AMe6ok9x"
      },
      "source": [
        "After designing the model, you will need to train it. In order to train, you will need to pick a number of `epoch` (iteration), which `optimizer` to use (from  `keras.optimizers`), a `loss` function, and some `metrics`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYsKzVh0pKRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b2c324-06ee-4513-8af7-ee512903587e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m358/358\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 1s/step - accuracy: 0.1133 - loss: 110.7433 - val_accuracy: 0.1323 - val_loss: 2.7545\n",
            "Epoch 2/5\n",
            "\u001b[1m235/358\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2:40\u001b[0m 1s/step - accuracy: 0.1787 - loss: 2.6683"
          ]
        }
      ],
      "source": [
        "myEpochs = 5\n",
        "myOptimizer = 'adam'\n",
        "myLoss = 'sparse_categorical_crossentropy'\n",
        "myMetrics = ['accuracy']\n",
        "myModel.compile(loss = myLoss, optimizer = myOptimizer, metrics = myMetrics)\n",
        "history = myModel.fit(train_ds, validation_data=validation_ds, epochs = myEpochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDLqsjlkoNay"
      },
      "source": [
        "Next, you need to create (1) a plot of training and validation `loss` and (2) a plot of training and validation `accuracy`. These plots might give you some insights about your model performance and possibility of overfitting.\n",
        "\n",
        "Report the performance of your architecture on the validation set in a `confusion matrix`. Make comments on the performance by answering the following questiosns:\n",
        "- How well do you think your architecture is doing (overall accuracy)?\n",
        "- Where did it makes mistake most?\n",
        "- Which classes can be improved?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FhpZM_ErHqn"
      },
      "outputs": [],
      "source": [
        "# Your evaluation code here\n",
        "def plot_training_history(history):\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history['loss'], label='Training Loss')\n",
        "  plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "  plt.title('Model Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "  plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVsQNmPZo9KP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model_on_validation(validation_ds, model):\n",
        "    true_labels = []\n",
        "    predictions = []\n",
        "    for images, labels in validation_ds:\n",
        "        preds = model.predict(images, verbose=0)\n",
        "        predicted_labels = np.argmax(preds, axis=1)\n",
        "        true_labels.extend(labels.numpy())\n",
        "        predictions.extend(predicted_labels)\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "    disp.plot(cmap=plt.cm.Blues, xticks_rotation=45)\n",
        "    plt.title('Confusion Matrix on Validation Set')\n",
        "    plt.show()\n",
        "    return cm\n",
        "\n",
        "evaluate_model_on_validation(validation_ds, myModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJF5CTcOl2Fy"
      },
      "source": [
        "# Step 3: Use a Pre-trained Network with Transfer Learning\n",
        "Now that you have a your own custom model and some baseline performance, let's see if you can improve the performance using transfer learning and a pre-trained model. You may use any pre-trained model EXCEPT ones that already provided such as `Xception`, `MobileNet`, `EfficientNetB6`. Keep in mind that each pre-trained model may expect a different input shape, so adjust the size of your training images accordingly.\n",
        "\n",
        "Make sure you report the design of this architecture by answer the same questions 1-4 in Step 3.\n",
        "\n",
        "Hint: use `ImageNet` as weights when load the pre-train network, then add a `GlobalAveragePooling2D` and an output layer with `softmax` activation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJlRl1i-l970"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "input_shape = (img_height, img_width, 3)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "model = models.Sequential([base_model,\n",
        "                           layers.GlobalAveragePooling2D(),\n",
        "                           layers.Dense(len(class_names), activation='softmax')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_13nh6H9uXWm"
      },
      "source": [
        "Next, you will attempt to adapt this pre-trained model to your UVA Landmark dataset. It is recommended that you tried the two-phase training approach for your model:\n",
        "\n",
        "1.   Phase 1: Freeze the pre-train weights and only train the top layer\n",
        "2.   Phase 2: Train the entire network with much smaller learning rate (adapt the model to UVA data, but avoid destroying the transfered weights).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pChHvQqVvfPL"
      },
      "outputs": [],
      "source": [
        "# Phase 1 code here\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_phase1 = model.fit(train_ds, validation_data=validation_ds, epochs=5)\n",
        "plot_training_history(history_phase1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swnOoePTvhyH"
      },
      "outputs": [],
      "source": [
        "# Phase 2 code here\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history_phase2 = model.fit(train_ds, validation_data=validation_ds, epochs=5)\n",
        "plot_training_history(history_phase2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r_ODJoOq2pQ"
      },
      "outputs": [],
      "source": [
        "evaluate_model_on_validation(validation_ds, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmFvO-uNMi7S"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(validation_ds)\n",
        "overall_accuracy = accuracy * 100\n",
        "print(f\"Overall Validation Accuracy: {overall_accuracy:.2f}%\")\n",
        "\n",
        "if overall_accuracy >= 94:\n",
        "    print(\"Performance meets or exceeds 94%.\")\n",
        "elif overall_accuracy >= 84:\n",
        "    print(\"Performance meets or exceeds 84% but is below 94%.\")\n",
        "else:\n",
        "    print(\"Performance is below 84%.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0qwDhE9uHW2"
      },
      "source": [
        "Repeat the same reporting of performance using the confusion matrix:\n",
        "- Did this pre-trained network do better overall?\n",
        "- In which class it improve the accuracy from the above model?\n",
        "- Which class still has low performance?\n",
        "\n",
        "Typically, your network must have a reasonable performance of at least 84% overall accuracy to be considered successful in this domain. If your network achieves a accuracy of 94% or above on the validation set, you will also recieve a 10 bonus points, so keep trying!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPmNtIGVl-7F"
      },
      "source": [
        "# Step 4: Reflection\n",
        "\n",
        "Write at least a paragraph answering these prompts: How did your own network perform in comparison to the pre-trained one? What are the major differences between the architectures? Additionally, report on your experience implementing different models for this assignment (Was it hard/easy/fun?, from which part did you learn the most?)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YtdjSICw66_"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}